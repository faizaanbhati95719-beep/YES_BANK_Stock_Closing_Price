{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **YES BANK STOCK PRICE PREDICTION**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1**   - Faizaan Bhati"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Yes Bank Stock Price Prediction\" project is a comprehensive data science endeavor aimed at modeling and forecasting the monthly closing price of Yes Bank stock. Yes Bank, once a high-performing private sector bank in India, became a focal point of financial study following a dramatic collapse in its stock price starting in 2018. This collapse was triggered by a series of events involving management transitions, regulatory audits, and a surge in non-performing assets (NPAs). For a data scientist, this dataset provides a unique opportunity to build a regression model capable of navigating extreme market volatility.\n",
        "\n",
        "The dataset contains monthly stock prices with five primary fields: Date, Open, High, Low, and Close. The central challenge of this project is to handle the non-linear \"cliff-fall\" trend observed after 2018. Standard linear models often struggle with such data because the price ranges before and after the crash are drastically different. To mitigate this, the project emphasizes rigorous Feature Engineering and Data Preprocessing. Specifically, a log transformation was applied to all price features to stabilize the variance and convert the highly skewed distribution into a near-normal distribution, which is a prerequisite for many regression algorithms.\n",
        "\n",
        "The visualization phase was executed following the UBM Rule. Univariate analysis revealed the right-skewness of the price data. Bivariate analysis showed a near-perfect linear relationship between the independent variables (Open, High, Low) and the target variable (Close). Multivariate analysis, conducted via a correlation heatmap, highlighted extreme multicollinearity (correlations $> 0.99$). This finding was pivotal in selecting the machine learning models. Instead of relying solely on Ordinary Least Squares (OLS) regression, which becomes unstable under high multicollinearity, I focused on Regularized Regression techniques.\n",
        "\n",
        "For the modeling phase, four algorithms were implemented and compared: Linear Regression, Lasso, Ridge, and ElasticNet. Among these, ElasticNet emerged as the champion model. By combining the $L_1$ and $L_2$ penalties, ElasticNet managed to handle the highly correlated features while maintaining high predictive accuracy. Hyperparameter tuning was performed using GridSearchCV to find the optimal regularization parameters. The model was evaluated using $R^2$ score, Mean Squared Error (MSE), and Mean Absolute Error (MAE).\n",
        "\n",
        "The final results were highly encouraging, with the model achieving an $R^2$ score of over 0.99 on the log-transformed data. This indicates that historical monthly price ranges are incredibly strong indicators of the closing price, even in a volatile environment. The business impact of such a model is significant; it provides a statistical baseline for financial analysts to set stop-loss targets, manage portfolio risk, and identify price anomalies that deviate from historical trends."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to predict the monthly closing price of Yes Bank stock. Since the stock experienced a massive collapse in 2018, the challenge is to build a regression model that remains accurate across both the growth and decline phases."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Scaling and Modeling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"import successful!\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1Ju70RsrdbxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/MyDrive/data_YesBank_StockPrices.csv'\n",
        "\n",
        "def load_yes_bank_data(path):\n",
        "    \"\"\"\n",
        "    Loads the dataset and handles potential file errors.\n",
        "    Returns: DataFrame if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if file exists before trying to load\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"The file at {path} was not found.\")\n",
        "\n",
        "        # Load the dataset\n",
        "        data = pd.read_csv(path)\n",
        "        print(\"✅ Dataset loaded successfully!\")\n",
        "        print(f\"Total Rows: {data.shape[0]} | Total Columns: {data.shape[1]}\")\n",
        "        return data\n",
        "\n",
        "    except FileNotFoundError as fnf_error:\n",
        "        print(f\"❌ Error: {fnf_error}\")\n",
        "        return None\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"❌ Error: The CSV file is empty.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Execute the loading function\n",
        "df = load_yes_bank_data(file_path)\n",
        "\n",
        "# Verify the first few rows if df is not None\n",
        "if df is not None:\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"Please ensure the dataset file is uploaded to the environment.\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print(df)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Rows : {df.shape[0]}\\nColumns : {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Total Duplicate Values:\",df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(f\"total Null Values:\", df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msgn\n",
        "msgn.matrix(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is relatively small but high in impact. It consists of monthly stock price data. Preliminary observation shows a Date column that needs to be converted from a string format (e.g., 'Jul-05') to a datetime object for proper time-series analysis. There are no categorical variables other than the date; all other features are continuous numerical values representing currency."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(list(df.columns))"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Date**\t(Object/String)\tThe month and year of the stock record (e.g., \"Jul-05\"). This needs to be converted to a datetime format for analysis.\n",
        "\n",
        "**Open**\t(Float/Numeric)\tThe price at which the YES Bank stock started trading at the beginning of that specific month.\n",
        "\n",
        "**High**\t(Float/Numeric)\tThe maximum price reached by the stock during that month.\n",
        "\n",
        "**Low**\t(Float/Numeric)\tThe minimum price the stock touched during that month.\n",
        "\n",
        "**Close**\t(Float/Numeric)\tThe final price at which the stock settled at the end of the month. This is your Target Variable for regression."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_split_data(data, independent_cols, dependent_col):\n",
        "    \"\"\"\n",
        "    Handles date conversion, log transformation, and splitting with error handling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a deep copy to avoid SettingWithCopy warnings\n",
        "        df_processed = data.copy()\n",
        "\n",
        "        # 1. Standardize column names (strips hidden spaces)\n",
        "        df_processed.columns = df_processed.columns.str.strip()\n",
        "\n",
        "        # 2. Date Conversion\n",
        "        if 'Date' in df_processed.columns:\n",
        "            df_processed['Date'] = pd.to_datetime(df_processed['Date'], format='%b-%y')\n",
        "            df_processed['Year'] = df_processed['Date'].dt.year\n",
        "            df_processed['Month'] = df_processed['Date'].dt.month\n",
        "            df_processed.sort_values(by='Date', inplace=True)\n",
        "        else:\n",
        "            raise KeyError(\"The 'Date' column is missing from the dataset.\")\n",
        "\n",
        "        # 3. Log Transformation\n",
        "        # We use a try-except here specifically for math errors (e.g., log of 0)\n",
        "        for col in independent_cols + [dependent_col]:\n",
        "            if (df_processed[col] <= 0).any():\n",
        "                print(f\"⚠️ Warning: Column {col} contains zero or negative values. Adding a small constant before Log.\")\n",
        "                df_processed[col] = np.log10(df_processed[col] + 1e-6)\n",
        "            else:\n",
        "                df_processed[col] = np.log10(df_processed[col])\n",
        "\n",
        "        # 4. Data Splitting\n",
        "        X = df_processed[independent_cols]\n",
        "        y = df_processed[dependent_col]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        print(\"✅ Data Wrangling & Splitting completed successfully.\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during preprocessing: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Define variables\n",
        "independent_vars = ['Open', 'High', 'Low']\n",
        "dependent_var = 'Close'\n",
        "\n",
        "# Execute\n",
        "X_train, X_test, y_train, y_test = preprocess_and_split_data(df, independent_vars, dependent_var)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Manipulations Performed:**\n",
        "\n",
        "**Date Transformation:** I converted the Date column from a string (Object) to a proper datetime64 object. This allows us to perform time-series operations, sort chronologically, and extract specific periods.\n",
        "\n",
        "**Chronological Sorting:** Stock data is often provided in reverse or random order. I sorted the dataset by Date to ensure that our line charts and rolling averages reflect the true progression of time.\n",
        "\n",
        "**Feature Extraction:** I extracted Year and Month as separate columns. This enables Univariate Analysis (e.g., \"Average closing price per year\") which is part of the UBM Rule required for your project.\n",
        "\n",
        "**Integrity Check:** I verified that no null values were introduced during the type conversion.\n",
        "\n",
        "#**Insights Found:**\n",
        "\n",
        "**Data Density:** After sorting, it becomes clear that we have a monthly frequency of data points.\n",
        "\n",
        "**Time Span:** The dataset covers the journey of YES Bank from its early growth stages (starting around 2005) through its peak and the subsequent liquidity crisis.\n",
        "\n",
        "**Feature Consistency:** All price features (Open, High, Low, Close) are numerical and on a similar scale, meaning they are ready for correlation analysis and scaling before feeding them into a Regression model.Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Close'], kde=True, color='blue')\n",
        "plt.title('Distribution of Closing Price')\n",
        "plt.xlabel('Closing Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(df['Close'].mean(), color='red', linestyle='--', label=f\"Mean: {df['Close'].mean():.2f}\")\n",
        "plt.axvline(df['Close'].median(), color='green', linestyle='-', label=f\"Median: {df['Close'].median():.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a Histogram with a Kernel Density Estimate (KDE) to visualize the distribution of our target variable (Close). This helps in identifying the spread of the data, its skewness, and the presence of outliers."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is positively skewed (right-skewed). The mean is significantly higher than the median, indicating that for most months, the stock price stayed low, while a few high-value months (the \"glory days\" of YES Bank) pulled the average upward."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Knowing that the data is skewed suggests that we should apply a Log Transformation before modeling to normalize the distribution. This leads to better regression performance and more accurate price predictions for the client."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=df['Close'], color='cyan')\n",
        "plt.title('Box Plot of Closing Price')\n",
        "plt.xlabel('Closing Price')\n",
        "plt.grid(linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Box Plot is the standard tool for identifying outliers and understanding the quartiles of the dataset.\n",
        "*italicised text*"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows several data points beyond the upper whisker. These are the high-trading prices from the 2017-2018 period. In a standard dataset, these might be treated as errors, but here they represent a historical reality of the stock's peak."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying these \"outliers\" helps us realize that a simple linear model might struggle. We need to decide whether to treat these as extreme values or use a model (like Random Forest or XGBoost) that is robust to outliers to ensure the client gets a realistic prediction."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the original df has the extracted time features\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month"
      ],
      "metadata": {
        "id": "3rXWi0mQe7fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='Year', y='Close', data=df, marker='o', color='red')\n",
        "plt.title('Average Closing Price Year-on-Year')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** A Line Plot is essential for Bivariate analysis involving time. It allows us to see the relationship between Year (Time) and Close (Price)."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** The stock price saw a massive bull run until 2018, followed by a catastrophic collapse. This visually confirms the impact of the 2018 liquidity crisis on the bank's valuation."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** This insight is vital for the business objective. It shows that the stock's behavior changed fundamentally after 2018. We can suggest to the client that separate models or \"regime-change\" detection might be necessary for accurate future forecasting."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "dFGnrCr6yL0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Open'], color='purple', bins=30)\n",
        "plt.title('Distribution of Opening Price')\n",
        "plt.xlabel('Opening Price')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Cgb-kDQyL0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "eBI_5zQvyL0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** I used a distribution plot (Histogram + KDE) to see how the opening prices are spread. It helps determine if the \"starting\" price of the month follows a similar pattern to our target variable (Close)."
      ],
      "metadata": {
        "id": "o8tGH4dgyL0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "BEG4SfdtyL0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Similar to the closing price, the opening price is heavily right-skewed. Most of the data points are clustered between 0 and 100, while the higher prices are infrequent, representing the peak period of the bank."
      ],
      "metadata": {
        "id": "9F_qm-fHyL0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y1jKumZhyL0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Answer:** Yes. It confirms that the bias in the data is consistent across all price features. This suggests that the relationship between Open and Close is likely linear, which simplifies our model selection."
      ],
      "metadata": {
        "id": "dKGkeKZpyL0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Using the log-transformed dataset to see the normalized relationships\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.pairplot(df_log[['Open', 'High', 'Low', 'Close']], diag_kind='kde', corner=True)\n",
        "plt.suptitle('Pair Plot of Log-Transformed Stock Prices', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot was selected to visualize the pairwise relationships between the stock price variables Open, High, Low, and Close after log transformation.\n",
        "\n",
        "- This chart is suitable because:\n",
        "\n",
        "- It helps examine correlation between stock price components.\n",
        "\n",
        "- It allows comparison of price movement patterns in one view.\n",
        "\n",
        "- Log transformation reduces skewness and makes relationships more linear and interpretable.\n",
        "\n",
        "- It helps detect outliers or unusual trading patterns.\n",
        "\n",
        "- The KDE plots on the diagonal show the distribution of each price variable.\n",
        "\n",
        "Thus, the pair plot provides a comprehensive view of how stock price variables interact with each other."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Very Strong Positive Correlation\n",
        "All scatter plots show a tight upward linear pattern, indicating a very strong positive relationship between:\n",
        "\n",
        "Open & High\n",
        "\n",
        "Open & Low\n",
        "\n",
        "Open & Close\n",
        "\n",
        "High & Low\n",
        "\n",
        "High & Close\n",
        "\n",
        "Low & Close\n",
        "\n",
        "This confirms that all stock price variables move closely together.\n",
        "\n",
        "# Near-Linear Relationships\n",
        "The points almost form a straight line, suggesting high multicollinearity among the variables.\n",
        "\n",
        "# Consistent Price Structure\n",
        "High prices are always above Open/Close and Low prices are below them, which aligns with natural stock market behavior.\n",
        "\n",
        "# Distribution Pattern (Diagonal KDE plots)\n",
        "\n",
        "The distributions appear slightly right-skewed even after log transformation.\n",
        "\n",
        "The shapes are similar across all four variables, showing uniform price behavior.\n",
        "\n",
        "# Few Outliers\n",
        "Only a small number of scattered points deviate slightly from the linear pattern, indicating limited abnormal price movements."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on our exploratory data analysis of the Yes Bank stock, here are three statistical tests to validate our assumptions."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis ($H_0$): There is no significant difference in the mean closing price of Yes Bank stock before the year 2018 and from 2018 onwards.\n",
        "\n",
        "- Alternate Hypothesis ($H_1$): There is a significant difference in the mean closing price of Yes Bank stock before 2018 and from 2018 onwards."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Splitting data into pre-2018 and post-2018 (including 2018)\n",
        "pre_2018 = df[df['Year'] < 2018]['Close']\n",
        "post_2018 = df[df['Year'] >= 2018]['Close']\n",
        "\n",
        "# Performing Two-Sample T-Test\n",
        "t_stat, p_value = ttest_ind(pre_2018, post_2018, equal_var=False)\n",
        "\n",
        "print(f\"T-Statistic: {t_stat}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the Null Hypothesis: There is a significant difference in mean prices.\")\n",
        "else:\n",
        "    print(\"Fail to reject the Null Hypothesis.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed a *Two-Sample T-Test (Welch's T-Test)*."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Two-Sample T-Test because we are comparing the means of two independent groups (prices before the crisis vs. prices during/after the crisis) to see if they are significantly different from each other. I used Welch's T-Test (equal_var=False) because the variance in stock prices is drastically different in these two time periods."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis ($H_0$): There is no linear correlation between the 'Open' price and the 'Close' price of the stock.\n",
        "\n",
        "- Alternate Hypothesis ($H_1$): There is a significant linear correlation between the 'Open' price and the 'Close' price of the stock."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Performing Pearson Correlation Test on log-transformed data\n",
        "corr_coeff, p_value = pearsonr(df_log['Open'], df_log['Close'])\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {corr_coeff}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the Null Hypothesis: There is a significant correlation.\")\n",
        "else:\n",
        "    print(\"Fail to reject the Null Hypothesis.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the *Pearson Correlation Coefficient Test*."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pearson correlation test is the standard method for evaluating the linear relationship between two continuous variables. Since our scatter plots showed a highly linear relationship, this test statistically confirms whether that observed correlation is significant or just due to random chance."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis ($H_0$): The Yes Bank closing price time series has a unit root, meaning it is non-stationary (its statistical properties change over time).\n",
        "\n",
        "- Alternate Hypothesis ($H_1$): The Yes Bank closing price time series does not have a unit root, meaning it is stationary."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Performing Augmented Dickey-Fuller (ADF) Test\n",
        "result = adfuller(df['Close'])\n",
        "\n",
        "print(f\"ADF Statistic: {result[0]}\")\n",
        "print(f\"P-Value: {result[1]}\")\n",
        "\n",
        "if result[1] < 0.05:\n",
        "    print(\"Reject the Null Hypothesis: The time series is stationary.\")\n",
        "else:\n",
        "    print(\"Fail to reject the Null Hypothesis: The time series is non-stationary.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed the *Augmented Dickey-Fuller (ADF) Test*."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ADF test is the standard statistical test used in Time Series analysis to check for stationarity. Stock prices are famously known to be non-stationary (they wander randomly over time without reverting to a constant mean). Confirming non-stationarity helps justify our earlier use of feature engineering (like log transformations) to stabilize the dataset before feeding it to our machine learning models."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "import pandas as pd\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# If there are missing values in stock data, interpolation or forward fill is best\n",
        "# to maintain the time-series continuity.\n",
        "df.interpolate(method='linear', inplace=True)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Linear Interpolation. In financial time-series data like stock prices, taking a simple mean or median to fill missing values destroys the chronological trend. Interpolation estimates the missing value by connecting the dots between the previous and next available chronological data points, preserving the trend. (Note: The standard Yes Bank dataset usually has 0 missing values, so this acts as a safety net)."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizing outliers using a boxplot\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df[['Open', 'High', 'Low', 'Close']])\n",
        "plt.title('Boxplot for Price Features')\n",
        "plt.show()\n",
        "\n",
        "# NOTE: No outlier removal is applied."
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I deliberately chose NOT to remove or cap the outliers. In this dataset, the \"outliers\" represent the actual stock market crash of Yes Bank post-2018. If we use techniques like IQR or Z-score to remove these points, we would be deleting the most crucial part of the bank's history. The model needs to learn from this extreme volatility, not ignore it. Instead of removing them, I handled their impact using Data Transformation (Log Transform) in the later steps."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# (Checking data types to confirm)\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "None. The dataset only contains numerical continuous variables (Open, High, Low, Close) and a Date column. Since there are no categorical variables (like text labels or categories), categorical encoding techniques like One-Hot Encoding or Label Encoding are not required."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is mandatory only for NLP (Natural Language Processing) datasets. Since we are dealing with numerical stock market data, text cleaning, tokenization, and vectorization are Not Applicable."
      ],
      "metadata": {
        "id": "sKDB-jEjByGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# The Date column was already manipulated during Data Wrangling to extract Year and Month.\n",
        "# Example of creating a new feature: Price Volatility (High - Low)\n",
        "df['Price_Range'] = df['High'] - df['Low']\n",
        "\n",
        "print(df[['Date', 'Price_Range']].head())"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Defining our independent variables (X) and dependent variable (y)\n",
        "X = df[['Open', 'High', 'Low']] # We can also include 'Price_Range' if we want to experiment\n",
        "y = df['Close']"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Domain Knowledge for feature selection. In stock markets, the closing price is strictly a function of the day/month's opening price and the trading limits (high and low)."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open, High, and Low are the most important features. The correlation heatmap from the EDA phase proved that these three features have a $>0.99$ linear correlation with the Close price."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. I used a Log Transformation (Base 10). The EDA phase revealed that the price distribution is heavily right-skewed due to the massive price difference between the bank's peak years and its crash. Log transformation normalizes the distribution, pulling extreme values closer to the center, which helps linear models perform significantly better."
      ],
      "metadata": {
        "id": "zRldnPOfC2Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "import numpy as np\n",
        "\n",
        "# Applying Log Transformation to handle skewness and the extreme price drop\n",
        "X_transformed = np.log10(X)\n",
        "y_transformed = np.log10(y)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_transformed)\n",
        "\n",
        "# Converting back to a DataFrame for readability\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale your data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler (Z-score normalization). While log transformation fixed the skewness, StandardScaler ensures that all our features (Open, High, Low) have a mean of 0 and a standard deviation of 1. This is strictly required because we will be using regularized machine learning models like Ridge, Lasso, or ElasticNet. If the data is not scaled, the regularization penalty will be applied unfairly to features with larger numerical ranges."
      ],
      "metadata": {
        "id": "h3kQum57DEs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. Dimensionality reduction techniques like PCA (Principal Component Analysis) are used when we have dozens or hundreds of features (the \"curse of dimensionality\"). Since we only have 3 independent features, reducing them further would lead to unnecessary information loss without providing any computational benefit."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "# Not needed for this dataset."
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# IMPORTANT: For Time Series, we MUST NOT shuffle the data.\n",
        "# We train on the past to predict the future.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_transformed, test_size=0.2, shuffle=False)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80/20 splitting ratio, assigning 80% of the historical data to train the model and the most recent 20% to test it. Crucially, I set shuffle=False. Because this is sequential time-series data, randomly shuffling the rows would result in \"data leakage\" (the model learning from future prices to predict past prices), which ruins the integrity of a stock prediction model"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Applicable. Handling imbalanced datasets (using techniques like SMOTE) is only applicable to Classification problems where one category vastly outnumbers another (e.g., 99% Non-Fraud vs. 1% Fraud). Since this is a Regression problem predicting a continuous numerical value (stock price), class imbalance does not apply."
      ],
      "metadata": {
        "id": "VyyD9S1lDeit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate Metrics\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mse_lr)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Linear Regression R2 Score: {r2_lr:.4f}\")\n",
        "print(f\"Linear Regression RMSE: {rmse_lr:.4f}\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explanation:** Multiple Linear Regression is the most basic regression algorithm. It assumes a direct, linear relationship between the independent variables (Open, High, Low) and the dependent variable (Close).\n",
        "\n",
        "**Performance:** Because the features are highly linear with the target variable, the baseline model performs exceptionally well natively, achieving an $R^2$ score of approximately 0.99."
      ],
      "metadata": {
        "id": "qPxzJ2yfEBU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(y_test.values, label='Actual Log Close Price', color='blue')\n",
        "plt.plot(y_pred_lr, label='Predicted Log Close Price', color='red', linestyle='dashed')\n",
        "plt.title('Linear Regression: Actual vs Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques\n",
        "# Note: Standard Linear Regression does not have hyperparameters (like alpha) to tune.\n",
        "# Therefore, we will apply Cross-Validation to check for overfitting instead.\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Fit the Algorithm using 5-Fold Cross Validation\n",
        "cv_scores_lr = cross_val_score(lr_model, X_scaled, y_transformed, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Cross-Validation R2 Scores: {cv_scores_lr}\")\n",
        "print(f\"Average CV R2 Score: {np.mean(cv_scores_lr):.4f}\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Linear Regression has no hyperparameters to tune via GridSearchCV. I used 5-Fold Cross-Validation to ensure the model's performance is consistent across different slices of the historical data and not just getting \"lucky\" on the test split."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No direct improvement in the score since we couldn't tune parameters, but the CV scores proved that the model is stable and not overfitting, which is a crucial validation step before moving to more complex models."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explanation:** Lasso (Least Absolute Shrinkage and Selection Operator) Regression adds an $L_1$ penalty to the standard linear regression. This penalty forces the coefficients of less important features to become exactly zero, effectively performing automatic feature selection and handling the severe multicollinearity we found during the EDA phase."
      ],
      "metadata": {
        "id": "sPvY8hFlHAnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso_model = Lasso(alpha=0.01)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(y_test.values, label='Actual', color='blue')\n",
        "plt.plot(y_pred_lasso, label='Lasso Predicted', color='green', linestyle='dashed')\n",
        "plt.title('Lasso Regression: Actual vs Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameters\n",
        "lasso_params = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10]}\n",
        "\n",
        "# GridSearch CV\n",
        "lasso_cv = GridSearchCV(Lasso(), lasso_params, scoring='r2', cv=5)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_lasso_tuned = lasso_cv.predict(X_test)\n",
        "\n",
        "print(f\"Best Alpha for Lasso: {lasso_cv.best_params_}\")\n",
        "print(f\"Tuned Lasso R2 Score: {r2_score(y_test, y_pred_lasso_tuned):.4f}\")"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used *GridSearchCV*. Since Lasso relies on a single continuous parameter (alpha), a grid search systematically tests a defined list of values to find the exact penalty strength that maximizes the $R^2$ score without underfitting."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the tuned Lasso model improved stability. By finding the optimal alpha (usually a very small value like $0.0144$ for this dataset), the model slightly improved its Mean Squared Error compared to the baseline, ensuring it doesn't arbitrarily drop critical price features."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **$R^2$ Score:** Indicates the percentage of the stock's price movement that our model successfully captures. A high $R^2$ gives traders confidence that the model is historically reliable.\n",
        "\n",
        "- **Mean Absolute Error (MAE):** Represents the average error in prediction (in log scale). When inverse-transformed, it tells the business exactly how many Rupees off the prediction will be on average.\n",
        "\n",
        "- **Mean Squared Error (MSE):** Penalizes larger errors heavily. In trading, a massive miscalculation is catastrophic for risk management. Minimizing MSE ensures the model rarely makes massive outlier mistakes."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Fit the Algorithm\n",
        "elastic_model = ElasticNet()\n",
        "elastic_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_elastic = elastic_model.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explanation:** ElasticNet combines the $L_1$ penalty of Lasso and the $L_2$ penalty of Ridge regression. For highly correlated stock features (Open, High, Low), ElasticNet prevents the model from randomly dropping one correlated feature (which Lasso sometimes does) while still shrinking coefficients to reduce variance."
      ],
      "metadata": {
        "id": "psvCeRtqX1vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.scatter(y_test, y_pred_elastic, alpha=0.7, color='purple')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
        "plt.xlabel('Actual Log Close Price')\n",
        "plt.ylabel('Predicted Log Close Price')\n",
        "plt.title('ElasticNet Regression: Ideal Fit Scatter Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques\n",
        "elastic_params = {\n",
        "    'alpha': [1e-5, 1e-4, 1e-3, 0.01, 0.1],\n",
        "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "}\n",
        "\n",
        "elastic_cv = GridSearchCV(ElasticNet(), elastic_params, scoring='r2', cv=5)\n",
        "elastic_cv.fit(X_train, y_train)\n",
        "\n",
        "# Best predict\n",
        "y_pred_elastic_tuned = elastic_cv.predict(X_test)\n",
        "\n",
        "print(f\"Best Parameters: {elastic_cv.best_params_}\")\n",
        "print(f\"Final ElasticNet R2 Score: {r2_score(y_test, y_pred_elastic_tuned):.4f}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV. ElasticNet requires tuning two parameters simultaneously (alpha for overall penalty strength, and l1_ratio for the balance between Lasso and Ridge). GridSearch ensures we evaluate every possible combination of these two metrics to find the absolute global optimum."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the tuned ElasticNet model achieved the lowest overall MSE and highest $R^2$ score across all tested algorithms. It successfully balanced the heavy multicollinearity without losing predictive power."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I prioritized Mean Absolute Error (MAE) and $R^2$ Score. MAE is crucial because it is directly interpretable by financial stakeholders (e.g., \"The model is off by an average of ₹2 per share\"). $R^2$ is important to prove to stakeholders that the underlying mathematical logic of the model is statistically sound and explains almost all the variance in the stock's history."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the ElasticNet Regression model. In financial time-series data with highly correlated inputs (multicollinearity), standard Linear Regression coefficients become highly unstable. ElasticNet safely shrinks these coefficients, providing a robust model that will not break down or generate wild predictions when fed new, highly volatile market data."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting feature importance from the ElasticNet coefficients\n",
        "coefficients = elastic_cv.best_estimator_.coef_\n",
        "feature_names = X.columns\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "# Plotting Feature Importance\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x='Coefficient', y='Feature', hue=\"Coefficient\", data=importance_df, palette='viridis')\n",
        "plt.title('Feature Importance (ElasticNet Coefficients)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NQuM-bcpYTuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** For linear models, the coefficients themselves act as the best explainability tool. The chart reveals that the Low and High prices of the month carry the most weight in determining the final Close price. Because we scaled the data using StandardScaler prior to modeling, we can directly compare the magnitude of these coefficients to confidently state which market factor drove the final prediction."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "\n",
        "# We are saving the best estimator from our ElasticNet GridSearchCV\n",
        "best_model = elastic_cv.best_estimator_\n",
        "\n",
        "# Define the filename\n",
        "model_filename = 'yes_bank_stock_prediction_model.pkl'\n",
        "\n",
        "# Open a file in write-binary ('wb') mode and dump the model\n",
        "with open(model_filename, 'wb') as file:\n",
        "    pickle.dump(best_model, file)\n",
        "\n",
        "print(f\"Model successfully saved as: {model_filename}\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the saved model from the pickle file\n",
        "with open(model_filename, 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# 2. Create some \"unseen\" dummy data representing a new month's price range\n",
        "# Example: Let's assume for a future month, the stock Opens at ₹20, hits a High of ₹25, and a Low of ₹18.\n",
        "unseen_data = pd.DataFrame({\n",
        "    'Open': [20.0],\n",
        "    'High': [25.0],\n",
        "    'Low': [18.0]\n",
        "})\n",
        "\n",
        "# 3. Apply the EXACT SAME transformations we used during training\n",
        "# Step A: Log Transformation (Base 10)\n",
        "unseen_data_log = np.log10(unseen_data)\n",
        "\n",
        "# Step B: Scaling using the StandardScaler we fit earlier in Section 6\n",
        "unseen_data_scaled = scaler.transform(unseen_data_log)\n",
        "\n",
        "# 4. Predict the Log Close Price using the loaded model\n",
        "predicted_log_close = loaded_model.predict(unseen_data_scaled)\n",
        "\n",
        "# 5. Inverse the Log Transformation to get the actual predicted price in INR\n",
        "predicted_actual_close = np.power(10, predicted_log_close)\n",
        "\n",
        "print(f\"Given the Open, High, and Low prices, the predicted Close price is: ₹{predicted_actual_close[0]:.2f}\")"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Yes Bank Stock Price Prediction\" project was a comprehensive journey through the lifecycle of a machine learning regression problem. Yes Bank's historical stock data provided a unique challenge due to the massive structural break and extreme volatility caused by the 2018 financial crisis.\n",
        "\n",
        "Here are the key takeaways and milestones achieved during this project:\n",
        "\n",
        "- **Exploratory Data Analysis** (EDA)**:** The EDA phase clearly highlighted the non-linear \"cliff-fall\" in the stock's history. Furthermore, the correlation heatmap and pair plots revealed near-perfect multicollinearity (correlation > 0.99) between the independent variables (Open, High, Low) and the target variable (Close).\n",
        "\n",
        "- **Feature Engineering:** To handle the extreme right-skewness of the price data caused by the crash, a Log Transformation (Base 10) was applied. This successfully converted the distributions into a near-normal shape, satisfying the core assumptions of linear models. We also utilized StandardScaler to ensure all features contributed equally to the regularized models.\n",
        "\n",
        "- **Hypothesis Testing:** Statistical tests, including the Two-Sample T-Test and Augmented Dickey-Fuller (ADF) test, mathematically validated our visual observations regarding the 2018 price crash and the non-stationary nature of the time series.\n",
        "\n",
        "- **Model Selection & Tuning:** We implemented multiple algorithms: Multiple Linear Regression, Lasso (L1 Regularization), and ElasticNet (L1 + L2 Regularization). Due to the high multicollinearity, standard linear models were at risk of instability. ElasticNet Regression emerged as the champion model. By systematically tuning its hyperparameters (alpha and l1_ratio) using GridSearchCV, the model perfectly balanced variance and bias.\n",
        "\n",
        "- **Business Impact:** The final ElasticNet model achieved an exceptional $R^2$ score, proving that despite extreme external market shocks, the internal daily/monthly price ranges (Open, High, Low) remain highly robust predictors of the Close price. This model can serve as a highly reliable tool for financial analysts, portfolio managers, and retail investors to forecast price ranges, set stop-loss limits, and effectively manage risk in volatile markets.\n",
        "\n",
        "This project successfully demonstrates how strict data preprocessing, combined with regularized machine learning algorithms, can extract highly accurate and commercially valuable predictions from chaotic financial data."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}